{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark Modules\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME']=\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\"\n",
    "os.environ['JAVA_HOME'] = \"C:\\java\\jdk1.8.0_211\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local[2] pyspark-shell\"\n",
    "\n",
    "# Append pyspark  to Python Path\n",
    "sys.path.append(\"C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\py4j-0.9-src.zip\")\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    print (\"Successfully imported Spark Modules\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "sc = SparkContext('local')\n",
    "words = sc.parallelize([\"scala\",\"java\",\"hadoop\",\"spark\",\"akka\"])\n",
    "print (words.count())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "- If you'd like to learn how to install PySpark and integrate it with IPython Notebook, this wonderful blog [https://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/] post will walk you through the steps.\n",
    "- The core data structure in Spark is a resilient distributed data set (RDD). \n",
    "- As the name suggests, an RDD is Spark's representation of a data set that's distributed across the RAM, or memory, of a cluster of many machines. \n",
    "- An RDD object is essentially a collection of elements we can use to hold lists of tuples, dictionaries, lists, etc. \n",
    "- Similar to a pandas DataFrame, we can load a data set into an RDD, and then run any of the methods accesible to that object.\n",
    "\n",
    "#### PySpark\n",
    "\n",
    "- While the Spark toolkit is in Scala, a language that compiles down to bytecode for the JVM, the open source community has developed a wonderful toolkit called PySpark that allows us to interface with RDDs in Python.\n",
    "- While the Spark toolkit is in Scala, a language that compiles down to bytecode for the JVM, the open source community has developed a wonderful toolkit called PySpark that allows us to interface with RDDs in Python. Thanks to a library called Py4J, Python can interface with Java objects (in our case RDDs). Py4J is also one of the tools that makes PySpark work.\n",
    "- Load the data set into an RDD. We're using the TSV version of FiveThirtyEight's data set. TSV files use a tab character (\"\\t\") as the delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark Modules\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME']=\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\"\n",
    "os.environ['JAVA_HOME'] = \"C:\\java\\jdk1.8.0_211\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local[2] pyspark-shell\"\n",
    "\n",
    "# Append pyspark  to Python Path\n",
    "sys.path.append(\"C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\py4j-0.9-src.zip\")\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    print (\"Successfully imported Spark Modules\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "sc = SparkContext('local')\n",
    "raw_data = sc.textFile(\"C:/Users/jyotibo/Python_Test/Python_code/Spark/daily_show.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR\\tGoogleKnowlege_Occupation\\tShow\\tGroup\\tRaw_Guest_List',\n",
       " '1999\\tactor\\t1/11/99\\tActing\\tMichael J. Fox',\n",
       " '1999\\tComedian\\t1/12/99\\tComedy\\tSandra Bernhard',\n",
       " '1999\\ttelevision actress\\t1/13/99\\tActing\\tTracey Ullman',\n",
       " '1999\\tfilm actress\\t1/14/99\\tActing\\tGillian Anderson']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Spark, the SparkContext object manages the connection to the clusters, and coordinates the running of processes on those clusters. More specifically, it connects to the cluster managers. \n",
    "- The cluster managers control the executors that run the computations.\n",
    "\n",
    "<img src=\"cluster-overview.png\" alt=\"Drawing\" align=\"left\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html#take\n",
    "- https://www.qubole.com/resources/pyspark-cheatsheet/\n",
    "\n",
    "- You may be wondering why, if an RDD resembles a Python list, we don't just use bracket notation to access elements in the RDD.\n",
    "\n",
    "- The answer is that Spark distributes RDD objects across many partitions, and the RDD object is specifically designed to handle distributed data. We can't rely on the standard implementation of a list for these reasons.\n",
    "\n",
    "- Spark offers many advantages over regular Python, though. For example, thanks to RDD abstraction, you can run Spark locally on your own computer. Spark will simulate distributing your calculations over many machines by automatically slicing your computer's memory into partitions.\n",
    "\n",
    "- Spark's RDD implementation also lets us evaluate code \"lazily,\" meaning we can postpone running a calculation until absolutely necessary. On the previous screen, Spark waited to load the TSV file into an RDD until raw_data.take(5) executed. When our code called raw_data = sc.textFile(\"dail_show.tsv\"), Spark created a pointer to the file, but didn't actually read it into raw_data until raw_data.take(5) needed that variable to run its logic.\n",
    "\n",
    "- The advantage of \"lazy\" evaluation is that we can build up a queue of tasks and let Spark optimize the overall workflow in the background. In regular Python, the interpreter can't do much workflow optimization. We'll see more examples of lazy evaluation later on.\n",
    "- https://blog.cloudera.com/blog/2014/09/how-to-translate-from-mapreduce-to-apache-spark/\n",
    "\n",
    "- The **key idea** to understand when working with Spark is data pipelining. \n",
    "- Every operation or calculation in Spark is essentially a series of steps that we can chain together and run in succession to form a pipeline.\n",
    "- Each step in the pipeline returns either \n",
    "    - a Python value (such as an integer), \n",
    "    - a Python data structure (such as a dictionary), \n",
    "    - an RDD object. \n",
    "- We'll start with the map() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map() :\n",
    "- The map(f) function applies the function f to every element in the RDD. \n",
    "- Because RDDs are iterable objects (like most Python objects), Spark runs function f on each iteration and returns a new RDD.\n",
    "- We'll walk through an example of a map function so you can get a better sense of how it works. \n",
    "- If you look carefully, you'll see that raw_data is in a format that's hard to work with. \n",
    "- While the elements are currently all strings, we'd like to convert each of them into a list to make the data more manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['YEAR', 'GoogleKnowlege_Occupation', 'Show', 'Group', 'Raw_Guest_List'],\n",
       " ['1999', 'actor', '1/11/99', 'Acting', 'Michael J. Fox'],\n",
       " ['1999', 'Comedian', '1/12/99', 'Comedy', 'Sandra Bernhard'],\n",
       " ['1999', 'television actress', '1/13/99', 'Acting', 'Tracey Ullman'],\n",
       " ['1999', 'film actress', '1/14/99', 'Acting', 'Gillian Anderson']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Call the RDD function `map()` to specify we want to apply the logic in the parentheses to every line in our data set.\n",
    "#2. Write a lambda function that splits each line using the tab delimiter (\\t), and assign the resulting RDD to `daily_show`.\n",
    "#3. Call the RDD function `take()` on `daily_show` to display the first five elements (or rows) of the resulting RDD.\n",
    "\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "daily_show.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the wonderful features of PySpark is the ability to separate our logic - which we prefer to write in Python - from the actual data transformation.\n",
    "- In the previous code cell, we wrote this lambda function in Python code.\n",
    "- Even though the function was in Python, we also took advantage of Scala when Spark actually ran the code over our RDD. This is the power of PySpark. \n",
    "- Without learning any Scala, we get to harness the data processing performance gains from Spark's Scala architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations and Actions :\n",
    "\n",
    "- **There are two types of methods in Spark:**\n",
    "    1. Transformations - map(), reduceByKey()\n",
    "    2. Actions - take(), reduce(), saveAsTextFile(), collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformations are lazy operations that always return a reference to an RDD object. \n",
    "- Spark doesn't actually run the transformations, though, until an action needs to use the RDD resulting from a transformation. \n",
    "- Any function that returns an RDD is a transformation, and any function that returns a value is an action. \n",
    "- These concepts will become more clear as we work through this lesson and practice writing PySpark code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immutability:\n",
    "- You may be wondering why we couldn't just split each string in place, instead of creating a new object daily_show. \n",
    "- In Python, we could have modified the collection element-by-element in place, without returning and assigning the results to a new object.\n",
    "- RDD objects are immutable, meaning that we can't change their values once we've created them. \n",
    "- In Python, list and dictionary objects are mutable (we can change their values), while tuple objects are immutable. \n",
    "- The only way to modify a tuple object in Python is to create a new tuple object with the necessary updates. \n",
    "- Spark uses the immutability of RDDs to enhance calculation speeds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduceByKey(f)**\n",
    "- combines tuples with the same key using the function we specify, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tally = daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 1),\n",
       " ('1999', 166),\n",
       " ('2000', 169),\n",
       " ('2001', 157),\n",
       " ('2002', 159),\n",
       " ('2003', 166),\n",
       " ('2004', 164),\n",
       " ('2005', 162),\n",
       " ('2006', 161),\n",
       " ('2007', 141),\n",
       " ('2008', 164),\n",
       " ('2009', 163),\n",
       " ('2010', 165),\n",
       " ('2011', 163),\n",
       " ('2012', 164),\n",
       " ('2013', 166),\n",
       " ('2014', 163),\n",
       " ('2015', 100)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tally.take(tally.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike pandas, Spark knows nothing about column headers, and didn't set them aside. We need a way to remove the element ('YEAR', 1) from our collection.\n",
    "- The only way to remove that tuple is to create a new RDD object that doesn't have it.\n",
    "- Spark comes with a filter(f) function that creates a new RDD by filtering an existing one for specific criteria.\n",
    "- If we specify a function f that returns a binary value, True or False, the resulting RDD will consist of elements where the function evaluated to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_year(line):\n",
    "    if line[0]=='YEAR':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "filtered_daily_show = daily_show.filter(lambda line: filter_year(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark improves on Hadoop's turnaround time significantly:\n",
    "- https://www.quora.com/What-are-the-advantages-of-DAG-directed-acyclic-graph-execution-of-big-data-algorithms-over-MapReduce-I-know-that-Apache-Spark-Storm-and-Tez-use-the-DAG-execution-model-over-MapReduce-Why-Are-there-any-disadvantages/answer/Tathagata-Das?share=1&srid=umKP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actor', 596),\n",
       " ('comedian', 103),\n",
       " ('television actress', 13),\n",
       " ('film actress', 21),\n",
       " ('singer-lyricist', 2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_daily_show.filter(lambda line: line[1] != '') \\\n",
    "                   .map(lambda line: (line[1].lower(), 1)) \\\n",
    "                   .reduceByKey(lambda x,y: x+y) \\\n",
    "                   .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hamlet@0\\t\\tHAMLET',\n",
       " 'hamlet@8',\n",
       " 'hamlet@9',\n",
       " 'hamlet@10\\t\\tDRAMATIS PERSONAE',\n",
       " 'hamlet@29']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_hamlet = sc.textFile(\"C:/Users/jyotibo/Python_Test/Python_code/Spark/hamlet.txt\")\n",
    "raw_hamlet.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hamlet@0', '', 'HAMLET'],\n",
       " ['hamlet@8'],\n",
       " ['hamlet@9'],\n",
       " ['hamlet@10', '', 'DRAMATIS PERSONAE'],\n",
       " ['hamlet@29']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_hamlet = raw_hamlet.map(lambda line : line.split(\"\\t\"))\n",
    "split_hamlet.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lambda functions are great for writing quick functions we can pass into PySpark methods with simple logic. \n",
    "- They fall short when we need to write more customized logic, though. \n",
    "- Thankfully, PySpark lets us define a function in Python first, then pass it in. \n",
    "- Any function that returns a sequence of data in PySpark (versus a guaranteed Boolean value, like filter() requires) must use a *yield* statement to specify the values that should be pulled later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yield\n",
    "\n",
    "- https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do/231855#231855\n",
    "- To understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables.\n",
    "\n",
    "**Iterables**\n",
    "- When you create a list, you can read its items one by one. Reading its items one by one is called iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "mylist = [1, 2, 3]\n",
    "for i in mylist:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "mylist = [x*x for x in range(3)]\n",
    "for i in mylist:\n",
    "   print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Everything you can use \"for... in...\" on is an iterable; lists, strings, files...\n",
    "- These iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generators**\n",
    "- Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "mygenerator = (x*x for x in range(3))\n",
    "for i in mygenerator:\n",
    "   print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is just the same except you used () instead of []. \n",
    "- BUT, you cannot perform for i in mygenerator a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yield**\n",
    "- Yield is a keyword that is used like return, except the function will return a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object createGenerator at 0x000001FB5024E6D8>\n",
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def createGenerator(): \n",
    "        mylist = range(3)\n",
    "        for i in mylist:\n",
    "            yield i*i\n",
    "mygenerator = createGenerator() # create a generator\n",
    "print(mygenerator) # mygenerator is an object!\n",
    "\n",
    "for i in mygenerator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once.\n",
    "- To master yield, you must understand that when you call the function, the code you have written in the function body does not run. \n",
    "- The function only returns the generator object, this is a bit tricky :-)\n",
    "- Then, your code will continue from where it left off each time for uses the generator.\n",
    "\n",
    "**Now the hard part:**\n",
    "- The first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it'll return the first value of the loop. \n",
    "- Then, each other call will run the loop you have written in the function one more time, and return the next value, until there is no value to return.\n",
    "- The generator is considered empty once the function runs, but does not hit yield anymore. It can be because the loop had come to an end, or because you do not satisfy an \"if/else\" anymore\n",
    "\n",
    "**To summarize**: \n",
    "- yield is a Python technique that allows the interpreter to generate data on the fly and pull it when necessary, instead of storing it to memory immediately. Because of its unique architecture, Spark takes advantage of this technique to reduce overhead and improve the speed of computations.\n",
    "- Spark runs the named function on every element in the RDD and restricts it in scope. \n",
    "- Each instance of the function only has access to the object(s) you pass into the function, and the Python libraries available in your environment. \n",
    "- If you try to refer to variables outside the scope of the function or import libraries, those actions may cause the computation to crash. \n",
    "- That's because Spark compiles the function's code to Java to run on the RDD objects (which are also in Java).\n",
    "- **Finally**, not all functions require us to use yield; only the ones that generate a custom sequence of data do. \n",
    "- For map() or filter(), we use return to return a value for every single element in the RDD we're running the functions on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flatMap()**\n",
    "- flatMap() is different than map() because it doesn't require an output for every element in the RDD. \n",
    "- The flatMap() method is useful whenever we want to generate a sequence of values from an RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following code cell, we'll use the flatMap() method with the named function hamlet_speaks to check whether a line in the play contains the text HAMLET in all caps (indicating that Hamlet spoke). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hamlet@0', 'hamlet speaketh!'),\n",
       " ('hamlet@75', 'hamlet speaketh!'),\n",
       " ('hamlet@1004', 'hamlet speaketh!'),\n",
       " ('hamlet@9144', 'hamlet speaketh!'),\n",
       " ('hamlet@12313', 'hamlet speaketh!'),\n",
       " ('hamlet@12434', 'hamlet speaketh!'),\n",
       " ('hamlet@12760', 'hamlet speaketh!'),\n",
       " ('hamlet@12858', 'hamlet speaketh!'),\n",
       " ('hamlet@14821', 'hamlet speaketh!'),\n",
       " ('hamlet@15261', 'hamlet speaketh!')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hamlet_speaks(line):\n",
    "    id = line[0]\n",
    "    speaketh = False\n",
    "    \n",
    "    if \"HAMLET\" in line:\n",
    "        yield id,\"hamlet speaketh!\"\n",
    "\n",
    "hamlet_spoken = split_hamlet.flatMap(lambda x: hamlet_speaks(x))\n",
    "hamlet_spoken.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write a named function filter_hamlet_speaks to pass into filter(). Apply it to split_hamlet to return an RDD with the elements containing the word HAMLET.\n",
    "##### Assign the resulting RDD to hamlet_spoken_lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hamlet@0', '', 'HAMLET'],\n",
       " ['hamlet@75', 'HAMLET', 'son to the late, and nephew to the present king.'],\n",
       " ['hamlet@1004', '', 'HAMLET'],\n",
       " ['hamlet@9144', '', 'HAMLET'],\n",
       " ['hamlet@12313',\n",
       "  'HAMLET',\n",
       "  '[Aside]  A little more than kin, and less than kind.']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_hamlet_speaks(line):\n",
    "    if \"HAMLET\" in line:\n",
    "        return line\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "hamlet_spoken_lines = split_hamlet.filter(lambda line: filter_hamlet_speaks(line))\n",
    "hamlet_spoken_lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark has two kinds of methods, transformations and actions. \n",
    "- While we've explored some of the transformations, we haven't used any actions other than take()\n",
    "- Whenever we use an action method, Spark forces the evaluation of lazy code. \n",
    "- If we only chain together transformation methods and print the resulting RDD object, we'll see the type of RDD (e.g. a PythonRDD or PipelinedRDD object), but not the elements within it. That's because the computation hasn't actually happened yet.\n",
    "- Even though Spark simplifies chaining lots of transformations together, it's good practice to use actions to observe the intermediate RDD objects between those transformations. \n",
    "- This will let you know whether your transformations are working the way you expect them to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count()**\n",
    "- The count() method returns the number of elements in an RDD. \n",
    "- count() is useful when we want to make sure the result of a transformation contains the right number of elements. \n",
    "- For example, if we know there should be an element in the resulting RDD for every element in the initial RDD, we can compare the counts of both to ensure they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet_spoken_lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect()**\n",
    "- We've used take() to preview the first few elements of an RDD, similar to the way we've use head() in pandas. \n",
    "- But what about returning all of the elements in a collection? We need to do this to write an RDD to a CSV, for example. \n",
    "- It's also useful for running some basic Python code over a collection without going through PySpark.\n",
    "\n",
    "Running .collect() on an RDD returns a list representation of it. To get a list of all the elements in hamlet_spoken_lines, for example, we would write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hamlet_spoken_lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the number of elements in hamlet_spoken_lines, and assign the result to the variable named spoken_count.\n",
    "- Grab the 101st element in hamlet_spoken_lines (which has the list index 100), and assign that list to spoken_101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hamlet@58626', 'HAMLET', \"Why, then, 'tis none to you; for there is nothing\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spoken_count = 0\n",
    "spoken_101 = list()\n",
    "\n",
    "spoken_count = hamlet_spoken_lines.count()\n",
    "spoken_collect = hamlet_spoken_lines.collect()\n",
    "spoken_101 = spoken_collect[101]\n",
    "spoken_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
